# Multimodal Density-Ratio Guided Diffusion

<p align="center">
  <img src="https://img.shields.io/badge/Status-Research%20Project-orange" alt="Status">
  <img src="https://img.shields.io/badge/Python-3.9+-blue" alt="Python">
  <img src="https://img.shields.io/badge/PyTorch-2.0+-red" alt="PyTorch">
  <img src="https://img.shields.io/badge/License-MIT-green" alt="License">
</p>

> **Status:** ğŸš§ Ongoing research project â€” active development  
> **Metrics:** Quantitative evaluation (FID/KID, CLIP/AudioCLIP/ImageBind similarity, alignment metrics) **coming soon**

## ğŸ“‹ Overview

This repository explores **guidance for frozen diffusion models** using a **learned logâ€‘density ratio** signal (the pointwise mutual information, PMI). We train a **density-ratio estimator** at multiple noise levels $t$ to approximate the **logâ€‘density ratio / PMI** term:

$$\log \frac{p(x_t,y_t)}{p(x_t)\,p(y_t)}$$

At sampling time, we add its gradient to the base score so the sampler prefers **corresponding pairs**.

### ğŸ¯ Key Features

- **No finetuning of base diffusion models** (we train a small densityâ€‘ratio head; base DDPM/SD/AudioLDM remain frozen)
- **Multiple density-ratio objectives** (Discriminator, DV, uLSIF, RuLSIF, KLIEP)
- **Two modality settings**:
  - Night â†’ Day image translation (DDPM)
  - Audio â†’ Image generation (AudioLDM2 + Stable Diffusion)
- **Latent space guidance** for pretrained models

## ğŸ–¼ï¸ Supported Modalities

### Night â†’ Day Image Translation
<p align="center">
  <img src="figures/night2day_dataset.png" alt="Night to Day Dataset Examples" width="80%">
  <br>
  <em>Examples of night-to-day image pairs used for training the density ratio estimator</em>
</p>

Two separately trained DDPMs (night/day) with a ratio model that learns correspondence across noise levels. The gradient nudges the "day" sampler toward the night input.

### Audio â†’ Image Generation
<p align="center">
  <img src="figures/ave_dataset.png" alt="AVE Dataset Examples" width="80%">
  <br>
  <em>Audio-Visual Event (AVE) dataset samples showing paired audio spectrograms and corresponding images</em>
</p>

Guidance in **latent space** using pretrained VAEs (AudioLDM2, SD). The ratio model is trained on **AVE video clips preprocessed into melâ€‘spectrograms and aligned image frames**; its gradient steers Stable Diffusion toward the audio input.

## ğŸ”¬ Technical Approach

### Density Ratio Learning

Let $q(x_t,y_t)=p(x_t,y_t)$ (joint) and $r(x_t,y_t)=p(x_t)p(y_t)$ (product of marginals), both at the **same** noise level $t$. We **do not estimate global mutual information** $I(X;Y)$; we directly estimate the **pointwise** logâ€‘ratio $\log\frac{q}{r}$ (PMI).

We learn a time-conditioned estimator $\hat{\ell}_\theta(x_t,y_t,t)$ such that:

$$\hat{\ell}_\theta \approx \log\frac{q}{r}$$

### Guided Sampling

During sampling for $x_t\mid y_t$, the guidance is:

$\nabla_{x_t}\log p(x_t\mid y_t) \approx \underbrace{s_\phi(x_t,t)}_{\text{unconditional score}} + \lambda\,\nabla_{x_t}\hat{\ell}_\theta(x_t,y_t,t)$

with an optional schedule $\lambda=\lambda(t)$ (e.g., scaled by $\sigma_t$).

## ğŸ“¦ Installation

### Prerequisites

- Python 3.9+
- CUDA 11.7+ (for GPU support)
- 16GB+ GPU memory recommended

### Environment Setup

```bash
# Create conda environment
conda create -n mm_guidance python=3.9 -y
conda activate mm_guidance

# Install PyTorch (adjust CUDA version as needed)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# Install dependencies
pip install einops tqdm soundfile librosa scikit-learn
pip install diffusers transformers accelerate safetensors
```

## ğŸš€ Quick Start

### Training

Train density-ratio (PMI) models for different modality pairs:

```bash
# Nightâ†”Day with discriminator loss
python src/train.py ddpm_night2day --loss_type disc --epochs 20

# Audioâ†’Image with RuLSIF (latent space)
python src/train.py sd_audioldm --loss_type rulsif --batch_size 16
```

### Inference

Generate guided samples:

```bash
# Guided nightâ†’day pairs (DDPM)
python src/inference.py ddpm_night2day \
    --loss_type disc \
    --guidance_scale 2.0 \
    --sampling_steps 100

# Audio-conditioned images (SD + AudioLDM2)
python src/inference.py sd_audioldm \
    --loss_type rulsif \
    --guidance_scale 1.5
```

## ğŸ“Š Implemented Density-Ratio Objectives

We parameterize a small network $T_\theta(x_t,y_t,t)$ and map it to a ratio or log-ratio:

| Method | Objective | Guidance Signal |
|--------|-----------|-----------------|
| **Discriminator** | Logistic classification: $\mathbb{E}_q[-\log\sigma(T)] + \mathbb{E}_r[-\log(1-\sigma(T))]$ | $\nabla_{x_t} T_\theta$ (Bayesâ€‘optimal logit â‰ˆ log(q/r)) |
| **Donskerâ€“Varadhan** | MI-style DV objective: maximize $\mathbb{E}_q[T] - \log \mathbb{E}_r[e^{T}]$; at optimum $Tâ‰ˆ\log(q/r)+const$ | $\nabla_{x_t} T_\theta$ |
| **uLSIF** | Direct ratio fitting: $\tfrac{1}{2} \mathbb{E}_r[w^2] - \mathbb{E}_q[w]$ | $\nabla_{x_t} \log w$ |
| **RuLSIF** | Relative ratio: $w_\alpha = \frac{q}{\alpha q + (1-\alpha) r}$ | $\nabla_{x_t} \log\left(\frac{w_\alpha}{1-\alpha w_\alpha}\right)$ |
| **KLIEP** | KL-based: $\max_\theta\; \mathbb{E}_q[\log w] \;\text{s.t.}\; \mathbb{E}_r[w]=1$ | $\nabla_{x_t} \log w$ |

## ğŸ“ Project Structure

```
data/                                # Dataset storage
â”œâ”€â”€ ave/                            # Audio-Visual Event dataset
â”œâ”€â”€ night2day/                      # Night/Day image pairs
â””â”€â”€ processed/                      # Preprocessed data

src/
â”œâ”€â”€ dataloaders/
â”‚   â”œâ”€â”€ ave.py                      # AVE (audioâ€“image) pairs
â”‚   â””â”€â”€ night2day.py                # Night/Day image pairs
â”œâ”€â”€ models/
â”‚   â””â”€â”€ mi_models.py                # Ratio/PMI estimators
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ density_ratio_losses.py     # Loss implementations
â”‚   â”œâ”€â”€ diffusion_schedule.py       # Schedules & time embeddings
â”‚   â”œâ”€â”€ score_computation.py        # Gradient computation
â”‚   â”œâ”€â”€ guided_sampler_native.py    # Native DDPM guidance
â”‚   â””â”€â”€ diffusion_model_loader.py   # Model loaders
â”œâ”€â”€ ddpm/
â”‚   â””â”€â”€ denoising_diffusion_pytorch/  # Local DDPM implementation
â”œâ”€â”€ train.py                         # Training script
â””â”€â”€ inference.py                     # Inference script

checkpoints/                         # Saved models
â”œâ”€â”€ {loss_type}/
â”‚   â””â”€â”€ {model_type}_{loss_type}_mi_model_best.pt

figures/                             # Dataset visualizations
â”œâ”€â”€ night2day_dataset.png
â””â”€â”€ ave_dataset.png
```

## ğŸ’¾ Model Checkpoints

- **DDPM models:** `src/ddpm/results/<domain>/<cfg>/<run_timestamp>/model-<k>.pt`
- **Ratio (PMI) models:** `checkpoints/{loss_type}/{model_type}_{loss_type}_mi_model_best.pt`

## ğŸ“ˆ Datasets

### Night/Day Dataset
- Paired night and day images for image-to-image translation
- Curated correspondences between different lighting conditions

### AVE (Audio-Visual Event) Dataset
- **Video** dataset of 4,143 tenâ€‘second clips across **28** event classes ([CVF Open Access](https://openaccess.thecvf.com/content_ECCV_2018/papers/Yapeng_Tian_Audio-Visual_Event_Localization_ECCV_2018_paper.pdf))
- We **preprocess** each clip into a melâ€‘spectrogram and one or more aligned **video frames** for training the ratio model
- Negatives for productâ€‘ofâ€‘marginals are generated via inâ€‘batch shuffling

## ğŸ—ºï¸ Roadmap

- [ ] **Quantitative Metrics**
  - [ ] FID/KID scores
  - [ ] CLIP/AudioCLIP/ImageBind similarity
  - [ ] Cross-modal alignment metrics
- [ ] **Ablation Studies**
  - [ ] Loss function comparison
  - [ ] Guidance scale analysis
  - [ ] Schedule ablations
- [ ] **Extended Modalities**
  - [ ] Text â†’ Image guidance
  - [ ] Video â†’ Audio generation
  - [ ] Cross-domain translation
- [ ] **Model Improvements**
  - [ ] Adaptive guidance scheduling
  - [ ] Multi-scale density ratios
  - [ ] Efficiency optimizations

## ğŸ“ Citation

If you use this code or find our work helpful, please cite:

```bibtex
@inprogress{mm_guidance_2025,
  title   = {Mutual-Information Guided Sampling for Multimodal Diffusion},
  author  = {Oubari, Fouad and Collaborators},
  year    = {2025},
  note    = {Work in progress}
}
```

## ğŸ¤ Contributing

We welcome contributions! Please feel free to submit issues or pull requests.

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- AVE dataset creators
- Authors of diffusers, AudioLDM2, and Stable Diffusion
- Density ratio estimation literature

---

<p align="center">
  <em>For questions or collaborations, please open an issue or contact the authors.</em>
</p>
